---
layout: post
title:  Станет ли "Small Data" следующим прорывом в области науки о данных?
permalink: /small-data-the-next-big-thing/
date:   2022-12-27 13:25:00 +0800
categories: Data Science
---
*Как прогнозирует Эндрю Нг, первопроходец в сфере искусственного интеллекта, следующее десятилетие будет ассоциироваться с дата-центричным подходом в сфере ИИ. Нет смысла использовать миллион сэмплов с высоким уровнем шума, если у нас есть 50 хорошо обработанных.*

![](https://miro.medium.com/max/1400/0*fOTXxmXuWchkoMnX)

*По всей видимости, мы стоим на пороге эры "малых данных" [Фото [Дэниела К. Ченга](https://unsplash.com/@danielkcheung?utm_source=medium&utm_medium=referral) из [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]*

Примерно последние два десятилетия мы жили в эпоху "Big Data". Поскольку цены на хранилища и вычислительные мощности постоянно падают, для генерации новых идей мы могли бы хранить и обрабатывать огромные объёмы данных. Успехи Google, Amazon и Facebook стали движущей силой множества крупных достижений в **анализе данных больших масштабов**, и для многих предприятий приоритетным направлением стало принятие решений на основе данных.

Мы видели гигантские нейронные сети с миллионами настраиваемых параметров. Обширные потоки информации из социальных сетей, обрабатываемые в режиме реального времени. Петабайты информации, по зёрнышку собранной с помощью высокочастотных сенсоров и пользовательских логов, хранятся в огромных цехах, заполненных серверами. Было множество научных прорывов огромной важности.

Без сомнения, эти **тренды в области больших данных** сохранятся. До тех пор, пока есть возможность собирать данные, будут находиться и новые способы их использования. Например, в сегменте обработки естественного языка достигнут определённый уровень зрелости, однако сегмент анализа видео всё ещё остаётся очень молодым и ожидает технологических достижений, которые позволят продвинуть разработки.

Тем не менее, за пределами Кремниевой долины существует целый мир, который обычно недооценивается. Миллионы компаний малых и средних размеров (а также других организаций) сталкиваются с проблемами, для решения которых нужна обработка данных. Им просто нужно извлечь ценную информацию из своих **небольших наборов данных**, используя самые современные технологии машинного обучения, но не связываясь с огромными датасетами. Возможно, настало их время.

Чтобы конкретизировать потенциальные области применения, давайте рассмотрим следующие несколько примеров:

* **Учёт затрат**: Прогнозирование затрат на оборудование, изготовленное на заказ
* **Здравоохранение**: Идентификация опухолей на рентгеновских снимках
* **Производство**: Автоматическое обнаружение дефектов на производственной линии

Важность таких примеров нельзя оспорить, как и ключевую роль, которую могут играть данные. Однако при решении таких задач не всегда можно получить миллиарды пунктов входных данных, особенно когда рассматриваются редкие дефекты или заболевания. Чтобы по максимуму использовать современные технологии машинного обучения, необходим другой подход.

![](https://miro.medium.com/max/1400/0*8MOXuJg_BlwwMi74)

*Одной из областей, которым будет полезно использование данных для обнаружения дефектов, является производство, однако количество релевантных образцов дефектов часто бывает слишком маленьким для эффективного машинного обучения [Фото [Mulyadi](https://unsplash.com/@mullyadii?utm_source=medium&utm_medium=referral) из [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]*

## Смена парадигмы?

Тут в дело вступает **Эндрю Нг**. Этот человек создал Google Brain, преподавал в Стэнфорде, был одним из основателей платформы Coursera (ему принадлежит чрезвычайно популярный курс "Machine Learning") и первым применил графические процессоры для машинного обучения. То есть можно с уверенностью сказать, что он имеет определённый уровень уважения. И когда он говорит о новой тенденции в сфере науки о данных, стоит прислушаться.

Эндрю утверждает, что для раскрытия всего потенциала искусственного интеллекта пора начать фокусироваться на *качестве данных*, и новое движение он назвал **дата-центричным ИИ**. В последние годы усилия сообщества были в основном *модель-центричными*, с акцентом на проектирование, тонкую настройку и совершенствование алгоритмов для различных задач (анализ текста, распознавание изображений и т.д.).

Модель-центричные исследования оказались очень плодотворными, и их кульминацией стало создание множества высококачественных архитектур. Однако для дальнейшего продвижения недостаточно одной только разработки и совершенствования алгоритмов. Для истинного прогресса **качество входных данных модели** должно соответствовать качеству трансформации.

Позже мы поговорим о дата-центричном подходе в ИИ более подробно, но сначала надо разобраться с модель-центричным, который в настоящее время доминирует в своей области.

## Model-centric AI

![](https://miro.medium.com/max/1400/1*VmN2ibVC-P__lgPJ0g8lHA.webp)

*В модель-центричной разновидности искусственного интеллекта данные принимаются как то, что есть по умолчанию. Основное внимание уделяется совершенствованию модели с целью получения максимального результата из фиксированного датасета [фото автора]*

Традиционно данные рассматриваются как нечто, получаемое алгоритмом на входе. Исследователь интересуется в основном тем, какой **алгоритм машинного обучения** выжмет максимальную отдачу из данных. Что нам лучше подходит – деревья с градиентным усилением или нейронные сети? Сколько нужно слоёв, какие функции активации, какой алгоритм градиентного спуска? Из-за множества доступных вариантов возникает множество проблем при определении подходящих архитектур. Большие датасеты позволяют преодолеть шум и пропуски в данных.

Эндрю Нг утверждает, что в сегменте модель-центричных ИИ сейчас достигнута **точка насыщения**. В процессе было решено множество открытых вопросов, в результате чего удалось провести расширенную оценку архитектур для различных задач. Например, алгоритм обработки естественного языка BERT Google обучался на основе английского языка. Для работы с другим языком вместо того, чтобы начинать с нуля, мы могли бы взять архитектуру BERT в качестве отправной точки, корректируя и адаптируя её в по ходу дела.

Благодаря лучшим умам и большому количеству экспериментов модель-центричный ИИ очень много нам дал. Для многих распространённых проблем мы сегодня имеем подходящие алгоритмы, которые **эмпирически доказали** свою эффективность. Подразумевается, что для определённых классов проблем мы можем использовать существующие модели, а не изобретать колесо заново для каждого случая, с которым мы сталкиваемся. Объединяем их с имеющимися инструментами, и вам больше не нужно быть экспертом по алгоритмам, чтобы развернуть готовый к работе искусственный интеллект.

Мы не хотим сказать, что модель-центричный ИИ является тупиковой веткой – продвижение в области алгоритмов будет продолжаться. Но для решения проблем искусственного интеллекта огромное значение имеют библиотеки с открытым исходным кодом и образцы архитектур. Что касается потенциала улучшения, то в настоящее время **от данных можно получить больше, чем от моделей**.

## Data-centric AI

![](https://miro.medium.com/max/1400/1*OEnGDv5A0D0i5yd1937ghw.webp)

*В этой разновидности разработки искусственного интеллекта модели остаются более или менее фиксированными. Основное внимание уделяется не моделям, а улучшению качества данных с целью добиться максимальной информативности небольших датасетов [фото автора]*

Несмотря на то, что каждый день добываются огромные объёмы данных **качество таких данных** может быть довольно низким (это знают все дата-сайентисты). Пропуски в данных, ошибки ввода или измерений, дубликаты, нерелевантные предикторы – всё это затрудняет обучение модели. Достаточно большие датасеты позволяют преодолеть такие препятствия, однако если датасет является одновременно небольшим *и* некачественным, вы находитесь на верном пути к катастрофе.

Кроме того, для работы часто бывают нужны только конкретные **подмножества данных**. Десять миллионов изображений здоровых лёгких или куча транзакций, в которых нет никаких признаков мошенничества, мало чем вам помогут в решении задач, связанных с болезнями или преступлениями. Даже если набор данных на первый взгляд кажется достаточно большим, мы часто сталкиваемся с серьёзным **[дисбалансом](https://towardsdatascience.com/precision-and-recall-a-comprehensive-guide-with-practical-examples-71d614e3fc43)**, так как значимых образцов для изучения оказывается мало.

В признании важности качества данных нет ничего нового – поговорка *"****мусор на входе = мусор на выходе****"* хорошо известна. Очистка данных обычно происходит один раз и зависит от изобретательности отдельных дата-сайентистов. Хуже того, до начала работы нельзя выяснить, исправление каких именно свойств данных (выбросы, пропуски, преобразования и т.д.) повлияют на производительность модели больше всего. Из-за этого мы получаем разочаровывающие серии проб и ошибок.

При использовании дата-центричного ИИ продвигается **систематический и методичный подход** **по улучшению качества данных**, который нацелен на сегменты данных, оказывающие наибольшее влияние на результат. Путём идентификации характерных особенностей, устранения шумов, анализа ошибок и их последовательной маркировки можно значительно повысить эффективность обучения. Главное здесь – генерализировать и автоматизировать такие процедуры.

До настоящего момента основное внимание было сосредоточено на совершенствовании моделей, а не **самих данных**. Дата-центричный подход к ИИ должен изменить это.

## Переход в направлении "small data"
Идея систематического совершенствования качества данных имеет смысл, но каких конкретно изменений следует ожидать? Самое важное – это переход к **небольшим хорошо структурированным датасетам** с упором на высококачественные данные и внятные примеры.

При этом можно использовать проверенные архитектуры с небольшими изменениями, более или менее позволяющими улучшить модель. В данном случае объектом внимания становятся сами данные. Если убрать недочёты из одной переменной (модели), анализ другой (данных) становится намного проще. Однако в больших датасетах довольно трудно разобраться. Для глубокого анализа человеком необходимы небольшие но всеобъемлющие наборы данных.

Переход на дата-центричный ИИ потребует существенного **изменения самой культуры.** Вместо того чтобы возиться со слоями и гиперпараметрами, мы будем тратить больше времени на маркировку и разделение наборов данных. Поскольку такие задачи большинству из нас не нравятся, к подобному изменению культуры не стоит относиться легкомысленно, даже если долгосрочная стратегия предполагает *автоматизацию* утомительных рутинных задач.

Помимо прочего, в дата-центричной разновидности ИИ продвигается системный подход в отношении совершенствования качества данных. Здесь можно выделить два основных направления:

* **Разработка инструментов для обнаружения несоответствий.** Чтобы масштабировать и генерализировать процессы совершенствования данных, нужно автоматизировать их выявление и очистку, отказавшись от трудоёмких процедур ручной очистки, где возникает много ошибок. Чрезвычайно важно, чтобы операции по очистке были последовательными и понятными.
* **Использование знаний в предметной области.** Чтобы точно интерпретировать передаваемую информацию, необходимы эксперты, способные тщательно изучать датасеты. Для получения максимальной отдачи от данных и для выявления возможных недостающих образцов необходимы точно рассчитанные характеристики и пороговые значения.

Центральное место в этих тенденциях занимает возможность интерпретации человеком, которая заставляет переходить к **небольшим и понятным датасетам.** Эндрю Нг утверждает, что 50 отличных образцов позволят так же хорошо хорошо обучить алгоритм ML, как и миллионы образцов с высоким уровнем шума. Очевидно, что для создания этих образцов нужно приложить существенные усилия, при этом каждый отдельный образец внесёт значимый вклад.

На практике желаемые данные не всегда легко получить. Что касается дополнения существующих данных (на основе анализа признаков мы можем определить варианты, быстро дающие результат), многообещающим направлением является технология **[Generative AI](https://towardsdatascience.com/bring-your-childhood-drawings-to-life-within-seconds-a-demo-of-metas-creative-ai-1e8695d9f3b6)**, позволяющая создавать синтетические данные, неотличимые от реальности. Основываясь на примерах и знаниях из предметной области, мы можем точно создавать искусственные примеры с нужными нам характеристиками, такие как изображение редкого дефекта или конкретного скачка фондового рынка.

Переход в сторону "small data" окажет значительное влияние на науку о данных. Этот подход открывает двери для решения многих проблем, которые не дают больших связанных наборов данных. Он позволяет генерировать высококачественные искусственные датасеты. И это хорошо согласуется с движением за "понятный ИИ" (Explainable AI), которое набирает популярность. Без преувеличений, это будет **фундаментальный прорыв** в данной сфере.

## Ключевые выводы
В последние десятилетия в области алгоритмов машинного обучения было совершено множество прорывов, поэтому можно сделать вывод, что точка насыщения достигнута. Уже есть множество библиотек (с открытым исходным кодом) и проверенных архитектур для решения различных задач, так что с исправлением моделей проблем возникнуть не должно. Учитывая этот факт, следующий прорыв может принести дата-центричный ИИ с акцентом на **систематический подход к улучшению качества данных** там, где это наиболее важно.

В современных подходах к обучению мы часто полагаемся на достаточно большие датасеты, позволяющие преодолеть шум и пропуски в данных. Однако из многих реальных проблем можно извлечь лишь **небольшие наборы данных**. Если тщательно подготовить репрезентативную выборку путём тщательного изучения примеров, для обучения высококачественных моделей может быть достаточно небольших датасетов. В достижении этой цели потребуется как человеческий опыт, так и методики систематического совершенствования, которые жизненно важны для реализации общего продвижения по всем фронтам.

В частности, ближайшее будущее в сфере науки о данных может принести активизацию внимания к таким видам деятельности, как (i) экспертный анализ, (ii) последовательная маркировка, (iii) устранение шумов, (iv) исправление ошибок, (v) разработка нового функционала и (v) [искусственная генерация данных](https://medium.com/mlearning-ai/top-data-science-trends-for-2022-what-do-ceos-have-to-say-86d3d3ea6e9f). При глубокой оценке небольших датасетов большее значение будут иметь **экспертные знания в предметной области и возможность интерпретации человеком**, однако долгосрочной целью является предоставление **систематических и автоматизированных решений** для исследований и повышения качества данных.

**Технология дата-центричного искусственного интеллекта** в ближайшем будущем может привести к значительным изменениям в повседневных задачах многих дата-сайентистов. Быстрые победы складываются в конкурентные преимущества, и мы подходим к моменту, когда большая часть побед может быть достигнута за счёт улучшения данных, а не алгоритмов.

https://towardsdatascience.com/is-small-data-the-next-big-thing-in-data-science-9acc7f24907f